声明1|线性回归估计器在所有无偏估计器中具有最小的方差。声明2|分配给 AdaBoost 组装的分类器的系数 α 始终是非负的。,真实，真实,假的，假的,真假,假，真,D
声明1| RoBERTa 预训练的语料库比 BERT 预训练的语料库大约大 10 倍。声明2| 2018 年的 ResNeXts 通常使用 tanh 激活函数。,真实，真实,假的，假的,真假,假，真,C
声明1|支持向量机，如逻辑回归模型，给出给定输入示例的可能标签的概率分布。声明2|当我们从线性内核转向高阶多项式内核时，我们期望支持向量总体上保持不变。,真实，真实,假的，假的,真假,假，真,B
机器学习问题涉及四个属性加一个类。每个属性有 3、2、2 和 2 个可能值。该类有 3 个可能的值。最多有多少个不同的例子？,12,24,48,72,D
截至 2020 年，哪种架构最适合对高分辨率图像进行分类？,卷积网络,图网络,全连接网络,RBF 网络,A
声明1|通过期望最大化算法的连续迭代，数据的对数似然总是会增加。声明2| Q 学习的一个缺点是它只能在学习者先验了解其行为如何影响环境时才能使用。,真实，真实,假的，假的,真假,假，真,B
假设我们已经计算了成本函数的梯度并将其存储在向量 g 中。给定梯度，一次梯度下降更新的成本是多少？,O(D),O(N),O(ND),O(ND^2),A
声明1|对于连续随机变量 x 及其概率分布函数 p(x)，对于所有 x，0 ≤ p(x) ≤ 1。声明2|决策树是通过最小化信息增益来学习的。,真实，真实,假的，假的,真假,假，真,B
考虑下面给出的贝叶斯网络。这个贝叶斯网络 H -> U <- P <- W 需要多少个独立参数？,2,4,8,16,C
随着训练示例的数量达到无穷大，根据该数据训练的模型将具有：,较低的方差,更高的方差,方差相同,以上都不是,A
声明1| 2D 平面中所有矩形的集合（包括非轴对齐的矩形）可以粉碎一组 5 个点。声明2|当 k = 1 时，k 最近邻分类器的 VC 维是无限的。,真实，真实,假的，假的,真假,假，真,A
_ 指的是既不能对训练数据建模也不能泛化到新数据的模型。,很合身,过拟合,欠拟合,上述所有的,C
声明1| F1 分数对于类别高度不平衡的数据集特别有用。声明2| ROC 曲线下面积是用于评估异常检测器的主要指标之一。,真实，真实,假的，假的,真假,假，真,A
声明1|反向传播算法学习具有隐藏层的全局最优神经网络。声明2|一条线的 VC 维数最多应该为 2，因为我可以找到至少一种 3 个点不能被任何线打碎的情况。,真实，真实,假的，假的,真假,假，真,B
高熵意味着分类中的分区是,pure,不纯粹,useful,无用,B
声明1|原始ResNet论文中使用的是Layer Normalization，而不是Batch Normalization。声明2| DCGAN 使用自注意力来稳定训练。,真实，真实,假的，假的,真假,假，真,B
在为特定数据集构建线性回归模型时，您会观察到具有相对较高负值的特征之一的系数。这表明,该特征对模型影响很大（应保留）,该特征对模型没有很强的影响（应该忽略）,如果没有附加信息，无法评论此功能的重要性,一切都无法确定。,C
对于神经网络，这些结构假设中哪一项对欠拟合（即高偏差模型）和过拟合（即高方差模型）之间的权衡影响最大：,隐藏节点数,学习率,权重的初始选择,使用常数项单位输入,A
对于多项式回归，这些结构假设中哪一项对欠拟合和过拟合之间的权衡影响最大：,多项式次数,我们是通过矩阵求逆还是梯度下降来学习权重,高斯噪声的假设方差,使用常数项单位输入,A
声明1|截至 2020 年，一些模型在 CIFAR-10 上的准确率超过 98%。声明2|原始 ResNet 并未使用 Adam 优化器进行优化。,真实，真实,假的，假的,真假,假，真,A
K-均值算法：,要求特征空间的维数不大于样本数,当 K = 1 时目标函数值最小,最小化给定数量的簇的类内方差,当且仅当选择初始均值作为某些样本本​​身时，收敛到全局最优值,C
声明1| VGGNet 的卷积核的宽度和高度比 AlexNet 的第一层内核更小。声明2|在批量归一化之前引入了数据相关的权重初始化程序。,真实，真实,假的，假的,真假,假，真,A
"以下矩阵的秩是多少？ A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",0,1,2,3,B
声明1|密度估计（例如使用核密度估计器）可用于执行分类。声明2|逻辑回归和高斯朴素贝叶斯（具有同一类协方差）之间的对应关系意味着两个分类器的参数之间存在一一对应的关系。,真实，真实,假的，假的,真假,假，真,C
假设我们要对空间数据（例如房屋的几何位置）进行聚类。我们希望生产许多不同尺寸和形状的簇。以下哪种方法最合适？,决策树,基于密度的聚类,基于模型的聚类,K-均值聚类,B
声明1|在 AdaBoost 中，错误分类示例的权重按相同的乘法因子上升。声明2|在 AdaBoost 中，第 t 个弱分类器在权重为 D_t 的训练数据上的加权训练误差 e_t 趋于作为 t 的函数而增加。,真实，真实,假的，假的,真假,假，真,A
MLE 估计通常是不可取的，因为,他们有偏见,他们有很高的方差,他们不是一致的估计者,以上都不是,B
梯度下降的计算复杂度是，,在 D 中呈线性,N 呈线性,D 中的多项式,取决于迭代次数,C
对多个决策树的输出进行平均有帮助_。,增加偏差,减少偏差,增加方差,减少方差,D
通过对所识别的特征子集应用线性回归获得的模型可能与在识别子集的过程结束时获得的模型不同,最佳子集选择,向前逐步选择,向前阶段明智的选择,上述所有的,C
神经网络：,优化凸目标函数,只能通过随机梯度下降进行训练,可以混合使用不同的激活函数,以上都不是,C
假设疾病 D 的发病率约为每 100 人 5 例（即 P(D) = 0.05）。让布尔随机变量 D 表示患者“患有疾病 D”，让布尔随机变量 TP 代表“测试呈阳性”。已知疾病 D 的检测非常准确，因为当您患有该疾病时检测呈阳性的概率为 0.99，而当您未患病时检测为阴性的概率为 0.97。什么是 P(TP)，即检测呈阳性的先验概率。,0.0368,0.473,0.078,以上都不是,C
声明1|通过径向基核函数映射到特征空间 Q 后，使用未加权欧氏距离的 1-NN 可能能够实现比原始空间更好的分类性能（尽管我们不能保证这一点）。声明2|感知器的 VC 维数小于简单线性 SVM 的 VC 维数。,真实，真实,假的，假的,真假,假，真,B
网格搜索的缺点是,它不能应用于不可微函数。,它不能应用于非连续函数。,实施起来很困难。,对于多元线性回归，它运行得相当慢。,D
根据各种线索预测一个地区的降雨量是一个______问题。,监督学习,无监督学习,聚类,以上都不是,A
关于回归，以下哪句话是错误的？,它将输入与输出联系起来。,它用于预测。,它可以用于解释。,它发现因果关系,D
以下哪一项是修剪决策树的主要原因？,节省测试期间的计算时间,节省存储决策树的空间,使训练集误差更小,避免训练集过度拟合,D
声明1|核密度估计器相当于在原始数据集中的每个点 Xi 处使用值 Yi = 1/n 执行核回归。声明2|学习决策树的深度可以大于用于创建树的训练示例的数量。,真实，真实,假的，假的,真假,假，真,B
假设您的模型过度拟合。以下哪项不是尝试减少过度拟合的有效方法？,增加训练数据量。,改进用于误差最小化的优化算法。,降低模型复杂度。,减少训练数据中的噪声。,B
声明1| softmax 函数常用于多类逻辑回归。声明2|非均匀 softmax 分布的温度会影响其熵。,真实，真实,假的，假的,真假,假，真,A
关于 SVM，以下哪一项是正确的？,对于二维数据点，线性 SVM 学习的分离超平面将是一条直线。,理论上，高斯核 SVM 无法对任何复杂的分离超平面进行建模。,对于 SVM 中使用的每个核函数，都可以获得等效的封闭形式基展开式。,SVM 中的过度拟合不是支持向量数量的函数。,A
以下哪项是给定贝叶斯网络 H -> U <- P <- W 描述的 H、U、P、W 的联合概率？ [注：作为条件概率的乘积],"P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",以上都不是,C
声明1|由于具有径向基核的 SVM 的 VC 维数是无限的，因此这种 SVM 必定比具有有限 VC 维数的多项式核的 SVM 差。声明2|具有线性激活函数的两层神经网络本质上是线性分离器的加权组合，在给定的数据集上进行训练；基于线性分隔符的增强算法也会找到线性分隔符的组合，因此这两种算法将给出相同的结果。,真实，真实,假的，假的,真假,假，真,B
声明1| ID3算法保证找到最优决策树。声明2|考虑密度 f() 处处非零的连续概率分布。值 x 的概率等于 f(x)。,真实，真实,假的，假的,真假,假，真,B
给定一个具有 N 个输入节点、无隐藏层、一个输出节点、具有熵损失和 Sigmoid 激活函数的神经网络，以下哪种算法（具有适当的超参数和初始化）可用于找到全局最优值？,随机梯度下降,小批量梯度下降,批量梯度下降,上述所有的,D
在线性模型中添加更多基函数，选择最可能的选项：,减少模型偏差,减少估计偏差,减少方差,不影响偏差和方差,A
考虑下面给出的贝叶斯网络。如果我们不对独立性或条件独立性 H -> U <- P <- W 做出假设，我们需要多少个独立参数？,3,4,7,15,D
分布外检测的另一个术语是？,异常检测,一类检测,训练-测试失配稳健性,背景检测,A
声明1|我们通过增强弱学习器 h 来学习分类器 f。 f 的决策边界的函数形式与 h 的相同，但参数不同。 （例如，如果 h 是线性分类器，则 f 也是线性分类器）。声明2|交叉验证可用于选择boosting中的迭代次数；此过程可能有助于减少过度拟合。,真实，真实,假的，假的,真假,假，真,D
声明1|高速公路网络是在 ResNets 之后引入的，并避开最大池化而采用卷积。声明2| DenseNet 通常比 ResNet 消耗更多内存。,真实，真实,假的，假的,真假,假，真,D
如果 N 是训练数据集中的实例数，则最近邻的分类运行时间为,O(1),O( N ),O(logN),O(N^2),B
声明1|最初的 ResNet 和 Transformer 都是前馈神经网络。声明2|原始的 Transformers 使用 self-attention，但原始的 ResNet 没有。,真实，真实,假的，假的,真假,假，真,A
声明1| RELU 不是单调的，但 sigmoid 是单调的。声明2|用梯度下降训练的神经网络以高概率收敛到全局最优值。,真实，真实,假的，假的,真假,假，真,D
神经网络中 sigmoid 节点的数值输出：,是无界的，包含所有实数。,是无界的，包含所有整数。,范围在 0 和 1 之间。,边界在 -1 和 1 之间。,C
以下哪项只能在训练数据线性可分时使用？,线性硬裕度 SVM。,线性逻辑回归。,线性软间隔支持向量机。,质心法。,A
以下哪些是空间聚类算法？,基于分区的聚类,K-均值聚类,基于网格的聚类,上述所有的,D
声明1|支持向量机构造的最大边缘决策边界在所有线性分类器中具有最低的泛化误差。声明2|我们从具有类条件高斯分布的生成模型中获得的任何决策边界原则上都可以使用 SVM 和次数小于或等于三的多项式核来重现。,真实，真实,假的，假的,真假,假，真,D
声明1|线性模型的 L2 正则化往往比 L1 正则化使模型更加稀疏。声明2|残差连接可以在 ResNet 和 Transformers 中找到。,真实，真实,假的，假的,真假,假，真,D
"假设我们要计算 P(H|E, F) 并且我们没有条件独立信息。以下哪组数字足以进行计算？","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)",P(H)、P(E|H)、P(F|H),"P(E, F), P(E|H), P(F|H)",B
当我们执行装袋操作时，以下哪项可以防止过度拟合？,使用放回抽样作为抽样技术,弱分类器的使用,使用不易过度拟合的分类算法,对每个经过训练的分类器进行验证的实践,B
声明1| PCA 和谱聚类（例如 Andrew Ng 的）对两个不同的矩阵执行特征分解。然而，这两个矩阵的大小是相同的。声明2|由于分类是回归的特例，逻辑回归是线性回归的特例。,真实，真实,假的，假的,真假,假，真,B
声明1|斯坦福情绪树库包含电影评论，而不是书评。声明2| Penn Treebank 已用于语言建模。,真实，真实,假的，假的,真假,假，真,A
"以下矩阵的零空间维数是多少？ A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",0,1,2,3,C
什么是支持向量？,距离决策边界最远的示例。,在 SVM 中计算 f(x) 所需的唯一示例。,数据质心。,SVM 中具有非零权重 αk 的所有示例。,B
声明1| Word2Vec 参数未使用受限玻尔兹曼机进行初始化。声明2| tanh 函数是非线性激活函数。,真实，真实,假的，假的,真假,假，真,A
如果您的训练损失随着轮数的增加而增加，以下哪项可能是学习过程中可能出现的问题？,正则化太低，模型过度拟合,正则化太高，模型欠拟合,步长太大,步长太小,C
假设疾病 D 的发病率约为每 100 人 5 例（即 P(D) = 0.05）。让布尔随机变量 D 表示患者“患有疾病 D”，让布尔随机变量 TP 代表“测试呈阳性”。已知疾病 D 的检测非常准确，因为当您患有该疾病时检测呈阳性的概率为 0.99，而当您未患病时检测为阴性的概率为 0.97。 P(D | TP) 是多少，即测试呈阳性时您患有疾病 D 的后验概率？,0.0495,0.078,0.635,0.97,C
声明1|传统的机器学习结果假设训练集和测试集是独立且同分布的。声明2| 2017 年，COCO 模型通常在 ImageNet 上进行预训练。,真实，真实,假的，假的,真假,假，真,A
"声明1|两个不同的核 K1(x, x0) 和 K2(x, x0) 在同一训练集上获得的边距值并不能告诉我们哪个分类器在测试集上表现更好。声明2| BERT的激活函数是GELU。",真实，真实,假的，假的,真假,假，真,A
以下哪一个是机器学习中的聚类算法？,期望最大化,CART,高斯朴素贝叶斯,先验,A
您刚刚完成了垃圾邮件分类决策树的训练，但它在训练集和测试集上的性能都异常糟糕。您知道您的实现没有错误，那么是什么原因导致了问题呢？,你的决策树太浅了。,你需要提高学习率。,你过度拟合了。,以上都不是。,A
K 折交叉验证为,与 K 呈线性关系,K 的二次方,立方K,K 的指数,A
声明1|工业规模的神经网络通常在 CPU 上训练，而不是在 GPU 上训练。声明2| ResNet-50 模型拥有超过 10 亿个参数。,真实，真实,假的，假的,真假,假，真,B
给定两个布尔随机变量 A 和 B，其中 P(A) = 1/2、P(B) = 1/3 和 P(A | ØB) = 1/4，P(A | B) 是多少？,1/6,1/4,3/4,1,D
人工智能带来的生存风险最常与以下哪位教授相关？,南多·德·弗雷塔斯,Y Ann l ECU内,斯图尔特·拉塞尔,吉滕德拉·马利克,C
声明1|最大化逻辑回归模型的可能性会产生多个局部最优值。声明2|如果数据的分布已知，则任何分类器都无法比朴素贝叶斯分类器做得更好。,真实，真实,假的，假的,真假,假，真,B
对于核回归，这些结构假设中哪一项对欠拟合和过拟合之间的权衡影响最大：,核函数是高斯函数还是三角形函数还是盒形函数,我们是否使用欧几里得度量、L1 度量还是 L∞ 度量,内核宽度,核函数的最大高度,C
声明1| SVM学习算法保证找到关于其目标函数的全局最优假设。声明2|通过径向基核函数映射到特征空间 Q 后，感知器可能能够实现比原始空间更好的分类性能（尽管我们不能保证这一点）。,真实，真实,假的，假的,真假,假，真,A
对于高斯贝叶斯分类器，这些结构假设中哪一项对欠拟合和过拟合之间的权衡影响最大：,我们是通过最大似然法还是梯度下降法来学习类中心,我们是否假设全类协方差矩阵或对角线类协方差矩阵,我们是否具有相同的类先验或根据数据估计的先验。,我们是否允许类具有不同的均值向量，或者强制它们共享相同的均值向量,B
声明1|当训练数据集较小时，更容易发生过度拟合。声明2|当假设空间较小时，更容易发生过拟合。,真实，真实,假的，假的,真假,假，真,D
声明1|除了 EM 之外，梯度下降还可以用于对高斯混合模型进行推理或学习。声明 2 |假设属性数量固定，基于高斯的贝叶斯最优分类器可以在时间上与数据集中的记录数量呈线性学习。,真实，真实,假的，假的,真假,假，真,A
声明1|在贝叶斯网络中，连接树算法的推理结果与变量消除的推理结果相同。声明2|如果给定另一个随机变量 Z，两个随机变量 X 和 Y 是条件独立的，则在相应的贝叶斯网络中，给定 Z 时，X 和 Y 的节点是 d 分离的。,真实，真实,假的，假的,真假,假，真,C
给定心脏病患者的大量医疗记录，尝试了解是否可能存在不同的此类患者群体，我们可以针对这些患者制定单独的治疗方案。这是一个什么样的学习问题？,监督学习,无监督学习,(a) 和 (b) 两者,(a) 和 (b) 都不是,B
在 PCA 中你会做什么来获得与 SVD 相同的投影？,将数据转换为零均值,将数据转换为零中位数,不可能,都不是,A
声明1| 1-最近邻分类器的训练误差为0。 语句2|随着数据点数量增长到无穷大，MAP 估计接近所有可能先验的 MLE 估计。换句话说，只要有足够的数据，先验的选择就无关紧要。,真实，真实,假的，假的,真假,假，真,C
当使用正则化进行最小二乘回归时（假设可以精确地完成优化），增加正则化参数 λ 的值测试误差。,永远不会减少训练误差。,永远不会增加训练误差。,永远不会减少测试误差。,永远不会增加,A
以下哪一项最能描述区分性方法试图建模的内容？ （w是模型中的参数）,"p(y|x, w)","p(y,x)","p(w|x, w)",以上都不是,A
声明1|卷积神经网络的 CIFAR-10 分类性能可以超过 95%。声明2|神经网络集成不会提高分类精度，因为它们学习的表示是高度相关的。,真实，真实,假的，假的,真假,假，真,C
贝叶斯主义者和频率论者不同意以下哪一点？,在概率回归中使用非高斯噪声模型。,使用概率模型进行回归。,在概率模型中使用参数的先验分布。,高斯判别分析中类先验的使用。,C
声明1| BLEU 指标使用精度，而 ROGUE 指标使用召回率。声明2|隐马尔可夫模型经常用于对英语句子进行建模。,真实，真实,假的，假的,真假,假，真,A
声明1| ImageNet 具有各种分辨率的图像。声明2| Caltech-101 的图像比 ImageNet 更多。,真实，真实,假的，假的,真假,假，真,C
以下哪项更适合做特征选择？,Ridge,Lasso,(a) 和 (b) 两者,(a) 和 (b) 都不是,B
假设给您一个 EM 算法，该算法可以找到具有潜在变量的模型的最大似然估计。系统会要求您修改算法，以便它找到 MAP 估计值。您需要修改哪一个或哪些步骤？,期待,最大化,无需修改,Both,B
对于高斯贝叶斯分类器，这些结构假设中哪一项对欠拟合和过拟合之间的权衡影响最大：,我们是通过最大似然法还是梯度下降法来学习类中心,我们是否假设全类协方差矩阵或对角线类协方差矩阵,我们是否具有相同的类别先验或根据数据估计的先验,我们是否允许类具有不同的均值向量，或者强制它们共享相同的均值向量,B
"声明1|对于具有联合分布 p(x, y) 的任何两个变量 x 和 y，我们总是有 H[x, y] ≥ H[x] + H[y]，其中 H 是熵函数。声明2|对于某些有向图，道德化会减少图中存在的边的数量。",真实，真实,假的，假的,真假,假，真,B
以下哪项不是监督学习？,PCA,决策树,线性回归,朴素贝叶斯,A
声明1|神经网络的收敛取决于学习率。声明2| Dropout 将随机选择的激活值乘以零。,真实，真实,假的，假的,真假,假，真,A
"给定布尔随机变量 A、B 和 C，并且它们之间没有独立性或条件独立性假设，以下哪一项等于 P(A, B, C)？",P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
使用聚类可以最好地解决以下哪些任务。,根据各种线索预测降雨量,检测欺诈性信用卡交易,训练机器人解决迷宫问题,上述所有的,B
在线性回归中应用正则化惩罚后，您会发现 w 的一些系数被清零。可能使用了以下哪种处罚？,L0范数,L1范数,L2范数,A或B）,D
"A和B是两个事件。如果 P(A, B) 减少而 P(A) 增加，以下哪项是正确的？",P(A|B) 减少,P(B|A) 减少,P(B) 减少,以上全部,B
声明1|当学习一组固定的观测值的 HMM 时，假设我们不知道隐藏状态的真实数量（通常是这种情况），我们总是可以通过允许更多的隐藏状态来增加训练数据的可能性。声明2|协同过滤通常是对用户的电影偏好进行建模的有用模型。,真实，真实,假的，假的,真假,假，真,A
您正在为简单的估计任务训练线性回归模型，并注意到该模型与数据过度拟合。您决定添加 $\ell_2$ 正则化来惩罚权重。当你增加$\ell_2$正则化系数时，模型的偏差和方差会发生什么变化？,偏差增加；方差增加,偏差增加；方差减少,偏差减少；方差增加,偏差减少；方差减少,B
"哪个 PyTorch 1.8 命令生成 $10\times 5$ 高斯矩阵，每个条目 i.i.d.从 $\mathcal{N}(\mu=5,\sigma^2=16)$ 和 $10\times 10$ 均匀矩阵中采样，每个条目 i.i.d.从 $U[-1,1)$ 采样？","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,低=-1,高=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0.5) / 0.5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \texttt{2 * torch.rand(10,10) - 1}",C
声明1|对于 $x<0$，ReLU 的梯度为零，对于所有 $x$，Sigmoid 梯度 $\sigma(x)(1-\sigma(x))\le \frac{1}{4}$。声明2| sigmoid 具有连续梯度，ReLU 具有不连续梯度。,真实，真实,假的，假的,真假,假，真,A
关于批量归一化，哪一项是正确的？,应用批量归一化后，层的激活将遵循标准高斯分布。,如果紧随其后的是批量归一化层，则仿射层的偏差参数将变得多余。,使用批量归一化时必须更改标准权重初始化。,批量归一化相当于卷积神经网络的层归一化。,B
假设我们有以下目标函数： $\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$ $\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$ 相对于 $w$ 的梯度是多少？,$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
关于卷积核，以下哪项是正确的？,将图像与 $\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$ 进行卷积不会改变图像,将图像与 $\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ 进行卷积不会改变图像,将图像与 $\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$ 进行卷积不会改变图像,将图像与 $\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ 进行卷积不会改变图像,B
以下哪项是错误的？,语义分割模型预测每个像素的类别，而多类图像分类器预测整个图像的类别。,IoU（并集交集）等于 $96\%$ 的边界框可能被视为真阳性。,当预测的边界框不对应于场景中的任何对象时，它被认为是误报。,IoU（并集交集）等于 $3\%$ 的边界框可能会被视为假阴性。,D
以下哪项是错误的？,以下没有激活函数的全连接网络是线性的：$g_3(g_2(g_1(x)))$，其中 $g_i(x) = W_i x$ 和 $W_i$ 是矩阵。,"Leaky ReLU $\max\{0.01x,x\}$ 是凸的。",ReLU 的组合（例如 $ReLU(x) - ReLU(x-1)$）是凸的。,损失 $\log \sigma(x)= -\log(1+e^{-x})$ 是凹的,C
我们正在训练具有两个隐藏层的全连接网络来预测房价。输入为 $100$ 维，并具有多种特征，例如平方英尺数、家庭收入中位数等。第一个隐藏层有 $1000$ 激活值。第二个隐藏层有 $10$ 激活。输出是代表房价的标量。假设一个普通网络具有仿射变换，没有批量归一化，并且激活函数中没有可学习的参数，那么该网络有多少个参数？,111021,110010,111110,110011,A
声明1| sigmoid 函数 $\sigma(x)=(1+e^{-x})^{-1}$ 对 $x$ 的导数等于 $\text{Var}(B)$ 其中 $B \sim \text{Bern}(\sigma(x))$ 是伯努利随机变量。声明2|将神经网络每层的偏差参数设置为0会改变偏差-方差权衡，使得模型的方差增加，模型的偏差减小,真实，真实,假的，假的,真假,假，真,C
